---
layout: single
title:  "(2023.09.22) [사이언스칼럼] 대형언어모델(LLM) 시대의 도전과 기회"
---

LLM 춘추 전국시대다. 세계 최대 AI 모델과 데이터 공유 오픈소스 플랫폼 허깅페이스에서 운영하는 LLM 성능 순위를 매기는 '오픈 LLM 리더보드'의 톱 순위가 며칠을 멀다하고 바뀌고 있다. 8월에 한국의 스타트업 업스테이지의 LLM이 세계 1위를 차지하는 쾌거를 올렸다. 9월 초에는 아랍에미리트(UAE) 과학연구센터첨단기술연구위원회(ATRC)의 기술혁신연구소(TII) 팰컨 180B(매개변수 1800억 개) 모델이 리더보드 정상에 올랐다. AI의 변방인 UAE에서 GPT-3(매개변수 1750억개) 크기의 사전학습 LLM을 개발한 것이 놀랍다. ATRC의 TII는 지난 5월 상업용 오픈소스 사전학습 모델의 첫 사례 중 하나인 팰컨 40B을 정상에 올리면서 세계적인 주목을 받았다.

LLM 서막을 알린 GPT-3가 출시된 2020년과 비교하면 격세지감을 느끼지 않을 수 없다. 불과 1~2년 전만해도 LLM 개발은 막대한 자본력과 기술력을 가진 미국의 특정 기업만이 할 수 있는 일이라 생각됐다. 도대체 어떻게 한국의 스타트업과 UAE의 한 연구소가 허깅페이스 오픈 LLM 리더보드 정상을 차지할 수 있게 됐는지를 한번 살펴보자.

그동안 LLM 기술의 글로벌 민주화가 꾸준히 진행돼 왔다. 첫째, 지난 수년간 트랜스포머 아키텍쳐가 LLM의 핵심으로 확고히 자리를 잡고 있다. 소스코드도 공개돼서 누구나 쉽게 트랜스포머를 구현할 수 있다. 트랜스포머의 각 층을 넓고 깊게 쌓기만 하면 대형모델이 된다. 둘째, 더 중요한 것은 거대 모델을 학습시키기 위해선 방대한 양의 데이터가 필요하다. 그동안 오픈소스 커뮤니티의 노력으로 허깅페이스와 같은 오픈소스 공유 플랫폼을 통해서 학습용 데이터 셋을 쉽게 구할 수 있다. 일례로, 지난 4월 레드파자마 프로젝트에서 1.2조 개의 토큰을 공개했다. 이는 GPT-3 사전학습용 데이터 토큰 3000억 개의 4배에 해당한다.

오픈 LLM 리더보드 정상에 오른 모델이 세계 최고 성능 LLM임을 의미하지는 않는다. 리더보드 4가지 벤치마크 지표인 초등수준 과학질의(ARC), 상식 추론(HellaSwag), 언어 이해 종합능력(MMLU), 환각현상방지(TruthfulQA)에서 제일 높은 평균 점수를 얻었다는 것이다. 여전히 오픈 AI의 챗GPT와 GPT-4가 세계 최고의 성능이다. 리더보드에 모델을 올릴 때 사전학습(pretrained), 미세 조정(fine-tuned), 명령어(instruction) 조정, 강화학습(RL) 조정한 모델인지를 명시하도록 돼 있다. 현재, 오픈 LLM 리더보드에는 미세조정 또는 명령어 조정한 모델이 거의 대부분이고, 라마2와 팰컨과 같이 사전학습 모델도 몇 개 보인다. 필자도 시험 삼아 허깅페이스에 공개된 데이터 셋을 이용해서 사전학습된 라마2를 양자화 미세조정한 모델을 리더보더에 올려보았다.

이번 한국 토종 스타트업의 오픈 LLM 리더보드 왕좌 등극을 계기로 앞으로 우리나라 AI 기업과 연구소 이름의 모델이 리더보드에서 많이 보이기를 기대한다. 좀 더 많은 기업이나 연구소에서 리더보드 4개 지표 공략 전략을 세우고, 성능 좋은 사전학습 모델과 데이터를 찾고, 미세 또는 명령어 조정한 토종 모델을 리더보드 정상에 올리는 데 적극 참여해야 한다. 처음에는 쉽지는 않겠지만 역량을 집중하면 단기적으로 할 수 있는 일이다. 이러한 작은 성공 경험과 할 수 있다는 조직 문화가 축적되면 UAE ATRC TII의 팰컨과 같이 세계적으로 인정받는 한국 토종 사전학습 LLM 모델도 조만간 나오게 될 것이다.

LLM 서비스 골드러시가 시작됐다. 오픈 LLM 리더보드에 1위 하는 것과는 차원이 다른 치열한 경쟁이다. LLM이 기존 서비스에 접목되어 새로운 기능을 추가하는 보조 역할에서 언어 이해와 추론 능력을 바탕으로 인터넷과 외부 툴을 연계·조율하는 중추 엔진 역할로 빠르게 변하고 있다. 이러한 LLM 중심의 혁신적인 서비스 패러다임의 변화를 감지하고 준비하는 자만이 황금을 캘 수 있게 될 것이다. 

[기사원문링크](http://www.joongdo.co.kr/web/view.php?lcode=&series=&key=20230921010006668)

![images](./assets/images/2023-09-22.png)
